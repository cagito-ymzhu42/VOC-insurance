{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\CAGITO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.785 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import jieba\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "#加载专名发现后得到的词典\n",
    "jieba.load_userdict( 'data/wesure_extra_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#话术类词表加载\n",
    "taolu_file = \"data/stopwords_cn.txt\"\n",
    "taolu_list = [line.strip() for line in open(taolu_file, encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 创建的目录\n",
    "product_code = \"Jan_KEEPER\"\n",
    "path = product_code + \"_multigram_cluster_renew\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    os.mkdir(path + '/cluster_2nd')\n",
    "    os.mkdir(path + '/cluster_2nd_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142071\n"
     ]
    }
   ],
   "source": [
    "load_dict = []\n",
    "with open('data/JAN_samples_isFAQ_with_1st_label_2nd_label_0322.txt','r',encoding='utf-8') as inf:\n",
    "    for line in inf:\n",
    "        data_json = json.loads(line.strip())\n",
    "        if data_json[\"label_1st\"] == \"其他\":\n",
    "            load_dict.append(data_json)\n",
    "            \n",
    "print(len(load_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ws_user_id': '878da5995af3491f99ba0ff5926365bc',\n",
       " 'userid': 'wmLpClCAAAjdWvPHboLxS0IWtoV0BkaA',\n",
       " 'user_flag': 0,\n",
       " 'user_name': '未来的.路',\n",
       " 'session_time': '2021-01-19 14:14:45',\n",
       " 'create_time': '2021-01-19 15:12:27',\n",
       " 'direction': 0,\n",
       " 'message_type': 'text',\n",
       " 'is_faq': 0,\n",
       " 'label_1st': '其他',\n",
       " 'query': '[疑问]',\n",
       " 'session_id': '210119ce6ac44fbe9af931befde5f20b'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clean = []\n",
    "for dic in load_dict:\n",
    "    temp = dic[\"query\"]\n",
    "    for word in taolu_list:\n",
    "        temp = temp.replace(word,\"\")\n",
    "    #过滤空白词与无意义的开头字\n",
    "    temp = temp.strip(\" \")\n",
    "    temp = temp.strip(\"好\")\n",
    "    temp = temp.strip(\"的\")\n",
    "    temp = temp.strip(\"们\")\n",
    "    if len(temp) >= 3:\n",
    "        dic[\"query_cleaned\"] = temp\n",
    "        dict_clean.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#话术类词表加载\n",
    "huashu_file = \"data/huashu_words.txt\"\n",
    "huashu_list = [line.strip() for line in open(huashu_file, encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram all num:\n",
      "144780\n",
      "Bigram top num:\n",
      "4170\n",
      "---------------------\n",
      "Trigram all num:\n",
      "203255\n",
      "Trigram top num:\n",
      "3062\n"
     ]
    }
   ],
   "source": [
    "bigram_dict = {}\n",
    "trigram_dict = {}\n",
    "multi_gram_clean_dict = []\n",
    "for dic in dict_clean:\n",
    "    sentence = dic[\"query_cleaned\"]\n",
    "    temp = list(jieba.lcut(sentence, cut_all=False))\n",
    "    new_temp = [x for x in temp if (x != \" \" and x.isdigit() == False)]\n",
    "    dic[\"unigram_cut\"] = new_temp\n",
    "    if len(new_temp) >= 3:\n",
    "        bi_temp = []\n",
    "        tri_temp = []\n",
    "        for j in range(len(new_temp)-2):\n",
    "            if new_temp[j] != new_temp[j+1]:\n",
    "                tri = new_temp[j] + new_temp[j+1]+ new_temp[j+2]\n",
    "                bi = new_temp[j] + new_temp[j+1]\n",
    "                if (bi in bigram_dict.keys())==False:\n",
    "                    bigram_dict[bi] = 1\n",
    "                else:\n",
    "                    bigram_dict[bi] += 1\n",
    "                bi_temp.append(bi)\n",
    "                if (tri in trigram_dict.keys())==False:\n",
    "                    trigram_dict[tri] = 1\n",
    "                else:\n",
    "                    trigram_dict[tri] += 1\n",
    "                tri_temp.append(tri)\n",
    "        if new_temp[-2] != new_temp[-1]:\n",
    "            bi_tail = new_temp[-2] + new_temp[-1]\n",
    "            if (bi_tail in bigram_dict.keys())==False:\n",
    "                bigram_dict[bi_tail] = 1\n",
    "            else:\n",
    "                bigram_dict[bi_tail] += 1\n",
    "            bi_temp.append(bi_tail)\n",
    "        bi_temp = list(set(bi_temp))\n",
    "        tri_temp = list(set(tri_temp))\n",
    "        dic[\"bigram_cut\"] = bi_temp\n",
    "        dic[\"trigram_cut\"]  = tri_temp\n",
    "        multi_gram_clean_dict.append(dic)\n",
    "    elif len(new_temp) == 2:\n",
    "        bi_temp = []\n",
    "        temp_bi = new_temp[0] + new_temp[1]\n",
    "        if (temp_bi in bigram_dict.keys())==False:\n",
    "            bigram_dict[temp_bi] = 1\n",
    "        else:\n",
    "            bigram_dict[temp_bi] += 1\n",
    "        bi_temp.append(temp_bi)\n",
    "        dic[\"bigram_cut\"]  = bi_temp\n",
    "        dic[\"trigram_cut\"]  = []\n",
    "        multi_gram_clean_dict.append(dic)\n",
    "    elif len(new_temp) == 1:\n",
    "        dic[\"bigram_cut\"]  = []\n",
    "        dic[\"trigram_cut\"]  = []\n",
    "        multi_gram_clean_dict.append(dic)\n",
    "    dic[\"unigram_cut\"] = [x for x in new_temp if (x in huashu_list) == False]\n",
    "        \n",
    "print(\"Bigram all num:\")\n",
    "print(len(bigram_dict))\n",
    "bigram_dict_top = {}\n",
    "for word in bigram_dict:\n",
    "    if bigram_dict[word] >= 10:\n",
    "        bigram_dict_top[word] = bigram_dict[word]\n",
    "print(\"Bigram top num:\")\n",
    "print(len(bigram_dict_top))\n",
    "print(\"---------------------\")\n",
    "print(\"Trigram all num:\")\n",
    "print(len(trigram_dict))\n",
    "trigram_dict_top = {}\n",
    "for word in trigram_dict:\n",
    "    if trigram_dict[word] >= 5:\n",
    "        trigram_dict_top[word] = trigram_dict[word]\n",
    "print(\"Trigram top num:\")\n",
    "print(len(trigram_dict_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['给 孩子 买 孩子买 给孩子 给孩子买', '什么 保险 什么 骗人 什么的 保险什么 什么保险', '有 什么 有什么', '哪 种 基本 重疾 哪种', '看 下 我看 看下 我看下']\n"
     ]
    }
   ],
   "source": [
    "for dic in multi_gram_clean_dict:\n",
    "    trigram_list = dic[\"trigram_cut\"]\n",
    "    trigram_top = []\n",
    "    for tri in trigram_list:\n",
    "        if tri in trigram_dict_top.keys():\n",
    "            trigram_top.append(tri)\n",
    "    bigram_list = dic[\"bigram_cut\"]\n",
    "    bigram_top = []\n",
    "    for bi in bigram_list:\n",
    "        if bi in bigram_dict_top.keys():\n",
    "            bigram_top.append(bi)\n",
    "    unigram_list = dic[\"unigram_cut\"]\n",
    "    multigram = unigram_list+bigram_top+trigram_top\n",
    "    dic[\"multigram_cut\"] = \" \".join(multigram) \n",
    "        \n",
    "multi_gram_list = []\n",
    "for dic in multi_gram_clean_dict:\n",
    "    multi_gram_list.append(dic[\"multigram_cut\"])\n",
    "print(multi_gram_list[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ws_user_id': '60f7086757d943978470dbd314dc9199',\n",
       " 'userid': 'wmLpClCAAAJQ-ixjll-P0_rkozXhGD3g',\n",
       " 'user_flag': 0,\n",
       " 'user_name': '许盼亮',\n",
       " 'session_time': '2021-01-18 17:27:08',\n",
       " 'create_time': '2021-01-18 18:10:49',\n",
       " 'direction': 0,\n",
       " 'message_type': 'text',\n",
       " 'is_faq': 1,\n",
       " 'label_1st': '其他',\n",
       " 'query': '我只是想一直给孩子买',\n",
       " 'session_id': '2101185390f1498c99d989f2ba54087c',\n",
       " 'query_cleaned': '给孩子买',\n",
       " 'unigram_cut': ['给', '孩子', '买'],\n",
       " 'bigram_cut': ['孩子买', '给孩子'],\n",
       " 'trigram_cut': ['给孩子买'],\n",
       " 'multigram_cut': '给 孩子 买 孩子买 给孩子 给孩子买'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_gram_clean_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gram_words = []\n",
    "multi_gram_time = []\n",
    "multi_gram_user_id = []\n",
    "multi_gram_user_name = []\n",
    "multi_gram_ws_user_id = []\n",
    "multi_gram_words_clean = []\n",
    "multi_gram_session_id = []\n",
    "for dic in multi_gram_clean_dict:\n",
    "    multi_gram_words.append(dic[\"query\"])\n",
    "    multi_gram_time.append(dic[\"create_time\"])\n",
    "    multi_gram_user_id.append(dic[\"userid\"])\n",
    "    multi_gram_user_name.append(dic[\"user_name\"])\n",
    "    multi_gram_ws_user_id.append(dic[\"ws_user_id\"])\n",
    "    multi_gram_words_clean.append(dic[\"query_cleaned\"])\n",
    "    multi_gram_session_id.append(dic[\"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 7785.328125 Seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer = CountVectorizer()\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer = TfidfTransformer()\n",
    "#第chat_clean一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(multi_gram_list))\n",
    "word = vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "start = time.process_time() \n",
    "clu_2000, n_clusters = KMeans(n_clusters=17).fit(tfidf), 17\n",
    "end = time.process_time() \n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice of i:17\n",
      "Running time: 4601.875 Seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer = CountVectorizer()\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer = TfidfTransformer()\n",
    "#第chat_clean一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(multi_gram_list))\n",
    "word = vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def train_cluster(train_vecs, start_k, end_k):\n",
    "    SSE = []\n",
    "    SSE_d1 = [] #sse的一阶导数\n",
    "    SSE_d2 = [] #sse的二阶导数\n",
    "    models = [] #保存每次的模型\n",
    "    for i in range(start_k, end_k):\n",
    "        kmeans_model = KMeans(n_clusters=i)\n",
    "        kmeans_model.fit(train_vecs)\n",
    "        SSE.append(kmeans_model.inertia_)  # 保存每一个k值的SSE值\n",
    "        #print('{} Means SSE loss = {}'.format(i, kmeans_model.inertia_))\n",
    "        models.append(kmeans_model)\n",
    "    # 求二阶导数，通过sse方法计算最佳k值\n",
    "    SSE_length = len(SSE)\n",
    "    for i in range(1, SSE_length):\n",
    "        SSE_d1.append((SSE[i - 1] - SSE[i]) / 2)\n",
    "    for i in range(1, len(SSE_d1) - 1):\n",
    "        SSE_d2.append((SSE_d1[i - 1] - SSE_d1[i]) / 2)\n",
    "\n",
    "    best_model = models[SSE_d2.index(max(SSE_d2)) + 1]\n",
    "    print(\"Choice of i:\" + str(SSE_d2.index(max(SSE_d2)) + 1 + start_k))\n",
    "    k_choice = SSE_d2.index(max(SSE_d2)) + 1 + start_k\n",
    "    return best_model, k_choice\n",
    "\n",
    "start = time.process_time() \n",
    "clu_2000, n_clusters = train_cluster(tfidf, start_k=16, end_k=20)\n",
    "end = time.process_time() \n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_choice = 20\n",
    "order_centroid = clu_2000.cluster_centers_.argsort()[:, ::-1]\n",
    "cluster_20=[]\n",
    "for i in range(n_clusters):\n",
    "    clusters=[]\n",
    "    for ind in order_centroid[i, :key_choice]:\n",
    "        clusters.append(word[ind])\n",
    "    cluster_20.append(clusters)\n",
    "    \n",
    "cluster_20_df = pd.DataFrame(cluster_20)\n",
    "cluster_20_df.to_excel(path + \"/\"+product_code+\"1st_cluster_\"+str(n_clusters)+\"_cluster_tags.xlsx\" , index=True)\n",
    "\n",
    "cluster_bag = []\n",
    "for line in cluster_20:\n",
    "    for word in line:\n",
    "        cluster_bag.append(word)\n",
    "        \n",
    "cluster_bag = list(set(cluster_bag))\n",
    "cluster_bag.sort(key=lambda x:len(x),reverse=True)\n",
    "\n",
    "cluster_bag = list(set(cluster_bag))\n",
    "cluster_bag.sort(key=lambda x:len(x),reverse=True)\n",
    "\n",
    "result_file = path + \"/cluster_\"+str(n_clusters)+\"_cluster_tags_bag.txt\"\n",
    "output = open(result_file, 'w', encoding=\"utf-8\")\n",
    "for row in cluster_bag:\n",
    "    output.write(row)\n",
    "    output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gram_words = []\n",
    "multi_gram_time = []\n",
    "multi_gram_user_id = []\n",
    "multi_gram_user_name = []\n",
    "multi_gram_ws_user_id = []\n",
    "multi_gram_words_clean = []\n",
    "multi_gram_session_id = []\n",
    "for dic in multi_gram_clean_dict:\n",
    "    multi_gram_words.append(dic[\"query\"])\n",
    "    multi_gram_time.append(dic[\"create_time\"])\n",
    "    multi_gram_user_id.append(dic[\"userid\"])\n",
    "    multi_gram_user_name.append(dic[\"user_name\"])\n",
    "    multi_gram_ws_user_id.append(dic[\"ws_user_id\"])\n",
    "    multi_gram_words_clean.append(dic[\"query_cleaned\"])\n",
    "    multi_gram_session_id.append(dic[\"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>query_cut</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>ws_user_id</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>我只是想一直给孩子买</td>\n",
       "      <td>给孩子买</td>\n",
       "      <td>给 孩子 买 孩子买 给孩子 给孩子买</td>\n",
       "      <td>2021-01-18 18:10:49</td>\n",
       "      <td>wmLpClCAAAJQ-ixjll-P0_rkozXhGD3g</td>\n",
       "      <td>许盼亮</td>\n",
       "      <td>60f7086757d943978470dbd314dc9199</td>\n",
       "      <td>2101185390f1498c99d989f2ba54087c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>这个比较精</td>\n",
       "      <td>比较精</td>\n",
       "      <td>比较 精</td>\n",
       "      <td>2021-01-06 14:56:03</td>\n",
       "      <td>wmLpClCAAAeQqRmdEdOHhrY3ARAnGwNA</td>\n",
       "      <td>lotus</td>\n",
       "      <td>8e00466899744411beb83471646f5c7d</td>\n",
       "      <td>210106aeb55b42d8ac4447c648a0c1a5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>要休息半年才能看到咋样的！</td>\n",
       "      <td>要休息半年咋样</td>\n",
       "      <td>要 休息 半年</td>\n",
       "      <td>2021-01-18 14:54:33</td>\n",
       "      <td>wmLpClCAAARxNfWJIaR9bd6mh7cURX5Q</td>\n",
       "      <td>热爱生活</td>\n",
       "      <td>b930507f35ed4b949fda2f83da33bada</td>\n",
       "      <td>210118ded15b47379ec62457ccc37cdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>有糖尿病</td>\n",
       "      <td>有糖尿病</td>\n",
       "      <td>有 糖尿病 有糖尿病</td>\n",
       "      <td>2021-01-05 12:08:41</td>\n",
       "      <td>wmLpClCAAAnnP4GYyVKZBMEztnD0aFng</td>\n",
       "      <td>相见恨晚</td>\n",
       "      <td>ab2e00135a31423daf288157e203c15b</td>\n",
       "      <td>2101052a2840404fbbec8a5d3723e51c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>微信聊吧</td>\n",
       "      <td>微信聊</td>\n",
       "      <td>微信 聊 微信聊</td>\n",
       "      <td>2021-01-08 10:56:48</td>\n",
       "      <td>wmLpClCAAAwsfbRgB90_FGMaOywZ_HiA</td>\n",
       "      <td>阿丽丝韵皮肤修复云云</td>\n",
       "      <td>8b4df27ecb79451b8d58377bd1c56eec</td>\n",
       "      <td>2101088a45e6418f986724c6301b3b8c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id          query query_cleaned            query_cut  \\\n",
       "0           0     我只是想一直给孩子买          给孩子买  给 孩子 买 孩子买 给孩子 给孩子买   \n",
       "1           0          这个比较精           比较精                 比较 精   \n",
       "2           0  要休息半年才能看到咋样的！       要休息半年咋样              要 休息 半年   \n",
       "3           0           有糖尿病          有糖尿病           有 糖尿病 有糖尿病   \n",
       "4           0           微信聊吧           微信聊             微信 聊 微信聊   \n",
       "\n",
       "                  time                           user_id   user_name  \\\n",
       "0  2021-01-18 18:10:49  wmLpClCAAAJQ-ixjll-P0_rkozXhGD3g         许盼亮   \n",
       "1  2021-01-06 14:56:03  wmLpClCAAAeQqRmdEdOHhrY3ARAnGwNA       lotus   \n",
       "2  2021-01-18 14:54:33  wmLpClCAAARxNfWJIaR9bd6mh7cURX5Q        热爱生活   \n",
       "3  2021-01-05 12:08:41  wmLpClCAAAnnP4GYyVKZBMEztnD0aFng        相见恨晚   \n",
       "4  2021-01-08 10:56:48  wmLpClCAAAwsfbRgB90_FGMaOywZ_HiA  阿丽丝韵皮肤修复云云   \n",
       "\n",
       "                         ws_user_id                        session_id  \n",
       "0  60f7086757d943978470dbd314dc9199  2101185390f1498c99d989f2ba54087c  \n",
       "1  8e00466899744411beb83471646f5c7d  210106aeb55b42d8ac4447c648a0c1a5  \n",
       "2  b930507f35ed4b949fda2f83da33bada  210118ded15b47379ec62457ccc37cdc  \n",
       "3  ab2e00135a31423daf288157e203c15b  2101052a2840404fbbec8a5d3723e51c  \n",
       "4  8b4df27ecb79451b8d58377bd1c56eec  2101088a45e6418f986724c6301b3b8c  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_clean_cluster = pd.DataFrame()\n",
    "chat_clean_cluster['cluster_id'] = clu_2000.labels_\n",
    "chat_clean_cluster[\"query\"] = multi_gram_words\n",
    "chat_clean_cluster[\"query_cleaned\"] = multi_gram_words_clean\n",
    "chat_clean_cluster[\"query_cut\"] = multi_gram_list\n",
    "chat_clean_cluster[\"time\"] = multi_gram_time\n",
    "chat_clean_cluster[\"user_id\"] = multi_gram_user_id\n",
    "chat_clean_cluster[\"user_name\"] = multi_gram_user_name\n",
    "chat_clean_cluster[\"ws_user_id\"] = multi_gram_ws_user_id\n",
    "chat_clean_cluster[\"session_id\"] = multi_gram_session_id\n",
    "chat_clean_cluster = chat_clean_cluster.sort_values(by = 'cluster_id',axis = 0,ascending = True)\n",
    "chat_clean_cluster = chat_clean_cluster.reset_index(drop=True)\n",
    "chat_clean_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(len(chat_clean_cluster)):\n",
    "    label_temp = []\n",
    "    cluster_label = int(chat_clean_cluster[\"cluster_id\"][i])\n",
    "    for word in cluster_20[cluster_label]:\n",
    "        if word in str(chat_clean_cluster[\"query_cut\"][i]):\n",
    "            label_temp.append(word)\n",
    "    labels.append(\" \".join(label_temp))\n",
    "\n",
    "chat_clean_cluster[\"label_keys\"] = labels\n",
    "\n",
    "chat_clean_cluster = chat_clean_cluster[[\"cluster_id\", \"label_keys\", \"query\", \"query_cleaned\",\\\n",
    "                                         \"query_cut\", \"time\", \"session_id\", \"user_id\", \"user_name\",\"ws_user_id\"]]\n",
    "\n",
    "chat_clean_cluster.to_excel(path + \"/\" + product_code + \"1st_cluster_result_\"+str(n_clusters)+\".xlsx\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tag = []\n",
    "for line in cluster_20:\n",
    "    cluster_tag.append(line[0])\n",
    "    \n",
    "chat_cluster_count = {}\n",
    "for i in range(len(chat_clean_cluster)):\n",
    "    cluster_id_temp = chat_clean_cluster[\"cluster_id\"][i]\n",
    "    if (cluster_id_temp in chat_cluster_count.keys()) == False:\n",
    "        chat_cluster_count[cluster_id_temp] = 1\n",
    "    else:\n",
    "        chat_cluster_count[cluster_id_temp] += 1\n",
    "        \n",
    "chat_cluster_count_list = []\n",
    "for i in range(len(chat_cluster_count)):\n",
    "    count = chat_cluster_count[i]\n",
    "    chat_cluster_count_list.append(count)\n",
    "    \n",
    "extra_cluster = chat_cluster_count_list.index(max(chat_cluster_count_list))\n",
    "\n",
    "cluster_2nd_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2nd_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 Recluster k: 17\n",
      "----------------------------------------------\n",
      "to be third cluster:\n",
      "0_10\n",
      "----------------------------------------------\n",
      "Choice of i:9\n",
      "Cluster: 1 Recluster k: 9\n",
      "Choice of i:8\n",
      "Cluster: 2 Recluster k: 8\n",
      "Choice of i:6\n",
      "Cluster: 3 Recluster k: 6\n",
      "Choice of i:8\n",
      "Cluster: 4 Recluster k: 8\n",
      "Choice of i:8\n",
      "Cluster: 5 Recluster k: 8\n",
      "Choice of i:7\n",
      "Cluster: 6 Recluster k: 7\n",
      "Choice of i:8\n",
      "Cluster: 7 Recluster k: 8\n",
      "Choice of i:7\n",
      "Cluster: 8 Recluster k: 7\n",
      "Choice of i:9\n",
      "Cluster: 9 Recluster k: 9\n",
      "Choice of i:8\n",
      "Cluster: 10 Recluster k: 8\n",
      "Choice of i:9\n",
      "Cluster: 11 Recluster k: 9\n",
      "Choice of i:6\n",
      "Cluster: 12 Recluster k: 6\n",
      "Choice of i:9\n",
      "Cluster: 13 Recluster k: 9\n",
      "Choice of i:8\n",
      "Cluster: 14 Recluster k: 8\n",
      "Choice of i:7\n",
      "Cluster: 15 Recluster k: 7\n",
      "Choice of i:6\n",
      "Cluster: 16 Recluster k: 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    chat_cluster_temp=chat_clean_cluster.loc[chat_clean_cluster['cluster_id']==i]\n",
    "    chat_cluster_temp=chat_cluster_temp.reset_index(drop=True)\n",
    "    if len(chat_cluster_temp) > 20:\n",
    "        user_chat_cut0 = list(chat_cluster_temp[\"query_cut\"])\n",
    "        tfidf_temp = transformer.fit_transform(vectorizer.fit_transform(user_chat_cut0))\n",
    "        word = vectorizer.get_feature_names()\n",
    "        if i == extra_cluster:\n",
    "            sub_clu_2000, k = KMeans(n_clusters=17).fit(tfidf_temp), 17\n",
    "        else:\n",
    "            start_k=5\n",
    "            end_k = 12\n",
    "            sub_clu_2000, k = train_cluster(tfidf_temp, start_k, end_k)\n",
    "        chat_cluster_temp['cluster_id_2nd'] = sub_clu_2000.labels_\n",
    "        chat_cluster_temp = chat_cluster_temp.sort_values(by = 'cluster_id_2nd', axis = 0, ascending=True)\n",
    "        chat_cluster_temp = chat_cluster_temp.reset_index(drop=True)\n",
    "        sub_n_clusters = k\n",
    "        if i == extra_cluster:\n",
    "            key_choice = 20\n",
    "        else:\n",
    "            key_choice = 10\n",
    "        order_centroids = sub_clu_2000.cluster_centers_.argsort()[:, ::-1]\n",
    "        cluster_sub=[]\n",
    "        for j in range(sub_n_clusters):\n",
    "            clusters=[]\n",
    "            for ind in order_centroids[j, :key_choice]:\n",
    "                clusters.append(word[ind])\n",
    "            cluster_sub.append(clusters)\n",
    "        #对于杂项聚类需要给出聚类簇标签        \n",
    "        sub_cluster_20_df = pd.DataFrame(cluster_sub)\n",
    "        if i == extra_cluster:\n",
    "            sub_cluster_20_df.to_excel(path +\"/cluster_2nd_tags/\"+product_code+\"extra_cluster_\"+str(i)+\"_\"+str(k)+\"_cluster_tags.xlsx\", index=True)\n",
    "        else:\n",
    "            sub_cluster_20_df.to_excel(path +\"/cluster_2nd_tags/\"+product_code+\"2nd_cluster_\"+str(i)+\"_\"+str(k)+\"_cluster_tags.xlsx\", index=True)\n",
    "        cluster_sub_bag = []\n",
    "        for j in range(len(cluster_sub)):\n",
    "            for word in cluster_sub[j]:\n",
    "                cluster_sub_bag.append(word)\n",
    "        cluster_sub_bag = list(set(cluster_sub_bag))\n",
    "        cluster_sub_bag.sort(key=lambda x:len(x),reverse=True)\n",
    "        sub_labels=[]\n",
    "        for j in range(len(chat_cluster_temp)):\n",
    "            label_temp = []\n",
    "            cluster_label = int(chat_cluster_temp[\"cluster_id_2nd\"][j])\n",
    "            for word in cluster_sub[cluster_label]:\n",
    "                if word in str(chat_cluster_temp[\"query_cut\"][j]):\n",
    "                    label_temp.append(word)\n",
    "            sub_labels.append(\" \".join(label_temp).replace(\"健告知\",\"健康告知\"))\n",
    "        chat_cluster_temp[\"label_keys_2nd\"] = sub_labels\n",
    "        chat_cluster_temp = chat_cluster_temp[['cluster_id', \"label_keys\",\"cluster_id_2nd\", \"label_keys_2nd\",  \"query\", \"query_cleaned\",\\\n",
    "                                         \"query_cut\", \"time\", \"session_id\", \"user_id\", \"user_name\",\"ws_user_id\"]]\n",
    "        cluster_2nd_results = cluster_2nd_results.append(chat_cluster_temp)\n",
    "        if i != extra_cluster:\n",
    "            chat_cluster_temp.to_excel(path+\"/cluster_2nd/2nd_recluster_\"+str(i)+\"_\"+str(sub_n_clusters)+\".xlsx\" , index=False)\n",
    "        else:\n",
    "            chat_cluster_temp.to_excel(path+\"/cluster_2nd/2nd_extra_recluster_\"+str(i)+\"_\"+str(sub_n_clusters)+\".xlsx\", index=False)\n",
    "        print(\"Cluster: \" + str(i) + \" Recluster k: \" + str(k))\n",
    "    else:\n",
    "        print(\"Too short to re-cluster: \" + str(i))\n",
    "    if i == extra_cluster:\n",
    "        chat_cluster_count = {}\n",
    "        for j in range(len(chat_cluster_temp)):\n",
    "            cluster_id_temp = chat_cluster_temp[\"cluster_id_2nd\"][j]\n",
    "            if (cluster_id_temp in chat_cluster_count.keys()) == False:\n",
    "                chat_cluster_count[cluster_id_temp] = 1\n",
    "            else:\n",
    "                chat_cluster_count[cluster_id_temp] += 1\n",
    "        chat_cluster_count_list = []\n",
    "        for j in range(k):\n",
    "            count = chat_cluster_count[j]\n",
    "            chat_cluster_count_list.append(count)\n",
    "        extra_cluster_temp = chat_cluster_count_list.index(max(chat_cluster_count_list))\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(\"to be third cluster:\")\n",
    "        recluster_index = i\n",
    "        print(str(recluster_index)+\"_\"+str(extra_cluster_temp))\n",
    "        print(\"----------------------------------------------\")\n",
    "\n",
    "cluster_2nd_results.to_excel(path + \"/\"+product_code+\"_2nd_cluster_result_\"+str(n_clusters)+\".xlsx\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>label_keys</th>\n",
       "      <th>cluster_id_2nd</th>\n",
       "      <th>label_keys_2nd</th>\n",
       "      <th>query</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>query_cut</th>\n",
       "      <th>time</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>ws_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>联系 我有</td>\n",
       "      <td>0</td>\n",
       "      <td>联系 联系你 我有</td>\n",
       "      <td>那先给我看一下吧，后面我有疑问再联系你</td>\n",
       "      <td>那先看后面我有疑问联系你</td>\n",
       "      <td>那先 看 后面 有 疑问 联系 后面我 我有 有疑问 联系你</td>\n",
       "      <td>2021-01-05 17:58:44</td>\n",
       "      <td>2101056c2b8647c7a126479ec98a6255</td>\n",
       "      <td>wmLpClCAAARsgxS7rI2BB08VDe37zUNA</td>\n",
       "      <td>元姐姐</td>\n",
       "      <td>20a5a5d26ef94435890a0317ccfa2a30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>时间</td>\n",
       "      <td>0</td>\n",
       "      <td>时间 有时间</td>\n",
       "      <td>你在忙吗，有时间和我聊好吗</td>\n",
       "      <td>你在忙有时间和我聊</td>\n",
       "      <td>在 忙 有 时间 聊 忙有 在忙 和我 有时间 你在 在忙有 忙有时间</td>\n",
       "      <td>2021-01-08 18:27:04</td>\n",
       "      <td>2101084c528a44dd93a36f984d45240c</td>\n",
       "      <td>wmLpClCAAAqEdNBy_DV_5eEDdcb6pO6A</td>\n",
       "      <td>丁庭武 天然蜂蜜</td>\n",
       "      <td>80276cae493d4bee9321ac5fc1138770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>联系</td>\n",
       "      <td>0</td>\n",
       "      <td>联系</td>\n",
       "      <td>好的[OK] 尽量中午12-1点左右联系。谢谢</td>\n",
       "      <td>尽量中午121点左右联系</td>\n",
       "      <td>尽量 中午 点 左右 联系 点左右 中午点</td>\n",
       "      <td>2021-01-15 15:53:52</td>\n",
       "      <td>2101152a734249a9ab259e5799999242</td>\n",
       "      <td>wmLpClCAAA7okM61tk8oev5u4kO5LbPg</td>\n",
       "      <td>AA优速快递 (UC优速)</td>\n",
       "      <td>d24ddf5d5f024c1483adc1dfc9eeffb9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>时间</td>\n",
       "      <td>0</td>\n",
       "      <td>时间 没时间</td>\n",
       "      <td>现在没时间</td>\n",
       "      <td>没时间</td>\n",
       "      <td>没 时间 没时间</td>\n",
       "      <td>2021-01-15 17:33:39</td>\n",
       "      <td>2101151c7876496a878d55e8730ab13b</td>\n",
       "      <td>wmLpClCAAASmk_LBt5XCX37TVNCmDo4A</td>\n",
       "      <td>lk</td>\n",
       "      <td>5e3af798120349468f5b37af8457f46b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>联系</td>\n",
       "      <td>0</td>\n",
       "      <td>联系 联系你</td>\n",
       "      <td>以后有毛病了，我就联系你吗</td>\n",
       "      <td>有毛病我就联系你</td>\n",
       "      <td>有 毛病 联系 就联系 我就 联系你 就联系你 我就联系</td>\n",
       "      <td>2021-01-15 10:00:08</td>\n",
       "      <td>21011595679e4d149190c2c9534bc440</td>\n",
       "      <td>wmLpClCAAAC20HzBYOSjODEh-332yaDQ</td>\n",
       "      <td>不要打扰我</td>\n",
       "      <td>3ac4b9a91d2046b6aece251e5b103e17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id label_keys  cluster_id_2nd label_keys_2nd  \\\n",
       "0           0      联系 我有               0      联系 联系你 我有   \n",
       "1           0         时间               0         时间 有时间   \n",
       "2           0         联系               0             联系   \n",
       "3           0         时间               0         时间 没时间   \n",
       "4           0         联系               0         联系 联系你   \n",
       "\n",
       "                     query query_cleaned                            query_cut  \\\n",
       "0      那先给我看一下吧，后面我有疑问再联系你  那先看后面我有疑问联系你       那先 看 后面 有 疑问 联系 后面我 我有 有疑问 联系你   \n",
       "1            你在忙吗，有时间和我聊好吗     你在忙有时间和我聊  在 忙 有 时间 聊 忙有 在忙 和我 有时间 你在 在忙有 忙有时间   \n",
       "2  好的[OK] 尽量中午12-1点左右联系。谢谢  尽量中午121点左右联系                尽量 中午 点 左右 联系 点左右 中午点   \n",
       "3                    现在没时间           没时间                             没 时间 没时间   \n",
       "4            以后有毛病了，我就联系你吗      有毛病我就联系你         有 毛病 联系 就联系 我就 联系你 就联系你 我就联系   \n",
       "\n",
       "                  time                        session_id  \\\n",
       "0  2021-01-05 17:58:44  2101056c2b8647c7a126479ec98a6255   \n",
       "1  2021-01-08 18:27:04  2101084c528a44dd93a36f984d45240c   \n",
       "2  2021-01-15 15:53:52  2101152a734249a9ab259e5799999242   \n",
       "3  2021-01-15 17:33:39  2101151c7876496a878d55e8730ab13b   \n",
       "4  2021-01-15 10:00:08  21011595679e4d149190c2c9534bc440   \n",
       "\n",
       "                            user_id      user_name  \\\n",
       "0  wmLpClCAAARsgxS7rI2BB08VDe37zUNA            元姐姐   \n",
       "1  wmLpClCAAAqEdNBy_DV_5eEDdcb6pO6A       丁庭武 天然蜂蜜   \n",
       "2  wmLpClCAAA7okM61tk8oev5u4kO5LbPg  AA优速快递 (UC优速)   \n",
       "3  wmLpClCAAASmk_LBt5XCX37TVNCmDo4A             lk   \n",
       "4  wmLpClCAAAC20HzBYOSjODEh-332yaDQ          不要打扰我   \n",
       "\n",
       "                         ws_user_id  \n",
       "0  20a5a5d26ef94435890a0317ccfa2a30  \n",
       "1  80276cae493d4bee9321ac5fc1138770  \n",
       "2  d24ddf5d5f024c1483adc1dfc9eeffb9  \n",
       "3  5e3af798120349468f5b37af8457f46b  \n",
       "4  3ac4b9a91d2046b6aece251e5b103e17  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cluster_2nd_results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103810\n"
     ]
    }
   ],
   "source": [
    "labels_new = []\n",
    "labels_new_set = []\n",
    "label1 = list(df[\"cluster_id\"])\n",
    "label2 = list(df[\"cluster_id_2nd\"])\n",
    "for i in range(len(label1)):\n",
    "    new_label = str(label1[i]) + \"-\" + str(label2[i])\n",
    "    if (new_label in labels_new_set) == False:\n",
    "        labels_new_set.append(new_label)\n",
    "    labels_new.append(new_label)\n",
    "print(len(labels_new))    \n",
    "df[\"cluster_new\"] = labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_labels = []\n",
    "for i in range(len(labels_new_set)):\n",
    "    chat_cluster_temp=df.loc[df['cluster_new']== labels_new_set[i]]\n",
    "    chat_cluster_temp=chat_cluster_temp.reset_index(drop=True)\n",
    "    cut_list = list(chat_cluster_temp[\"query_cut\"])\n",
    "    cut_list = [str(x).split(\" \") for x in cut_list]\n",
    "    word_bag = []\n",
    "    for line in cut_list:\n",
    "        for word in line:\n",
    "            if len(word) >= 2:\n",
    "                word_bag.append(word)\n",
    "    counter = Counter(word_bag)\n",
    "    word_counts_top = counter.most_common(20)\n",
    "    labels = [x[0] for x in word_counts_top]\n",
    "    word_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load(\"models/w2v.model\", mmap='r')\n",
    "vocab = list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_average_ranked(vecs, dim):\n",
    "    vec_result = []\n",
    "    for i in range(dim):\n",
    "        summ = 0\n",
    "        #加入词频排序信息\n",
    "        for j in range(len(vecs)):\n",
    "            summ += vecs[j][i] * (len(vecs) - j)\n",
    "        avg = (2 * summ) / (len(vecs) * (len(vecs) + 1))\n",
    "        vec_result.append(avg)\n",
    "    return vec_result\n",
    "\n",
    "def vec_average(vecs, dim):\n",
    "    vec_result = []\n",
    "    for i in range(dim):\n",
    "        summ = 0\n",
    "        #加入词频排序信息\n",
    "        for j in range(len(vecs)):\n",
    "            summ += vecs[j][i]\n",
    "        avg = summ / len(vecs) \n",
    "        vec_result.append(avg)\n",
    "    return vec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-5467a73d48b1>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec = model[word.replace(\"\\n\",\"\")]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "vec_labels = []\n",
    "for line in word_labels:\n",
    "    vecs = []\n",
    "    for word in line:\n",
    "        if word in vocab:\n",
    "            vec = model[word.replace(\"\\n\",\"\")]\n",
    "            vecs.append(vec)\n",
    "    vec_line = vec_average(vecs, 200)\n",
    "    vec_labels.append(vec_line)\n",
    "    \n",
    "print(len(vec_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice of i:31\n",
      "Running time: 4.015625 Seconds\n"
     ]
    }
   ],
   "source": [
    "def train_cluster(train_vecs, start_k, end_k):\n",
    "    SSE = []\n",
    "    SSE_d1 = [] #sse的一阶导数\n",
    "    SSE_d2 = [] #sse的二阶导数\n",
    "    models = [] #保存每次的模型\n",
    "    for i in range(start_k, end_k):\n",
    "        kmeans_model = KMeans(n_clusters=i)\n",
    "        kmeans_model.fit(train_vecs)\n",
    "        SSE.append(kmeans_model.inertia_)  # 保存每一个k值的SSE值\n",
    "        #print('{} Means SSE loss = {}'.format(i, kmeans_model.inertia_))\n",
    "        models.append(kmeans_model)\n",
    "    # 求二阶导数，通过sse方法计算最佳k值\n",
    "    SSE_length = len(SSE)\n",
    "    for i in range(1, SSE_length):\n",
    "        SSE_d1.append((SSE[i - 1] - SSE[i]) / 2)\n",
    "    for i in range(1, len(SSE_d1) - 1):\n",
    "        SSE_d2.append((SSE_d1[i - 1] - SSE_d1[i]) / 2)\n",
    "\n",
    "    best_model = models[SSE_d2.index(max(SSE_d2)) + 1]\n",
    "    print(\"Choice of i:\" + str(SSE_d2.index(max(SSE_d2)) + 1 + start_k))\n",
    "    k_choice = SSE_d2.index(max(SSE_d2)) + 1 + start_k\n",
    "    return best_model, k_choice\n",
    "\n",
    "start = time.process_time() \n",
    "clu_2000, n_clusters = train_cluster(vec_labels, start_k = 25, end_k = 55)\n",
    "end = time.process_time() \n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id_4clusters</th>\n",
       "      <th>cluster_id_original</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1-4</td>\n",
       "      <td>百万医疗险 退百万医疗险</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15-1</td>\n",
       "      <td>不想 不想买</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15-2</td>\n",
       "      <td>不想 不想买 投保 不想投保 续费 不想续费 不想投 不想续 不想入 不想上 不想保 不想交...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15-4</td>\n",
       "      <td>不想 不想保</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15-5</td>\n",
       "      <td>不想 不想交 原因</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id_4clusters cluster_id_original  \\\n",
       "0                     0                 1-4   \n",
       "1                     1                15-1   \n",
       "2                     1                15-2   \n",
       "3                     1                15-4   \n",
       "4                     1                15-5   \n",
       "\n",
       "                                                word  \n",
       "0                                       百万医疗险 退百万医疗险  \n",
       "1                                             不想 不想买  \n",
       "2  不想 不想买 投保 不想投保 续费 不想续费 不想投 不想续 不想入 不想上 不想保 不想交...  \n",
       "3                                             不想 不想保  \n",
       "4                                          不想 不想交 原因  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_clean_cluster = pd.DataFrame()\n",
    "chat_clean_cluster['cluster_id_4clusters'] = clu_2000.labels_\n",
    "chat_clean_cluster['cluster_id_original'] = labels_new_set\n",
    "chat_clean_cluster[\"word\"] = [\" \".join(x) for x in word_labels]\n",
    "chat_clean_cluster = chat_clean_cluster.sort_values(by = 'cluster_id_4clusters',axis = 0,ascending = True)\n",
    "chat_clean_cluster = chat_clean_cluster.reset_index(drop=True)\n",
    "chat_clean_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_clean_cluster.to_excel(path + \"/\"+product_code+\"_二次聚类_高频词表_重聚类结果_词向量平均.xlsx\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
